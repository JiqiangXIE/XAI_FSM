{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Interpretable_MLmodel_urban_flood_susceptibility"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.sparse.linalg import cg\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import shap\n",
    "import os\n",
    "import sys\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import sklearn.tree \n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "import shap\n",
    "import xgboost\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pylab as pl\n",
    "import pandas as pd\n",
    "from sklearn import datasets\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import LinearSVC\n",
    "import matplotlib.pyplot as plt\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from skopt import gp_minimize\n",
    "from skopt.space import Real, Integer\n",
    "from skopt.utils import use_named_args\n",
    "import math\n",
    "import torch\n",
    "# import tensorflow as tf\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "# from interpret_community.tabular_explainer import TabularExplainer\n",
    "from captum.attr import IntegratedGradients\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "import time\n",
    "import lime.lime_tabular\n",
    "import shap\n",
    "# from interpret_community.tabular_explainer import TabularExplainer\n",
    "from captum.attr import IntegratedGradients\n",
    "# import tensorflow as tf\n",
    "# from tensorflow.keras import backend as K\n",
    "import time\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "m=32\n",
    "random.seed(m)\n",
    "np.random.seed(m)\n",
    "torch.manual_seed(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import data\n",
    "train_data_original=pd.read_csv('train_data.csv')\n",
    "test_data_original=pd.read_csv('test_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize data\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "train_data = train_data_original.copy()  \n",
    "train_data.iloc[:, 1:9] = scaler.fit_transform(train_data_original.iloc[:, 1:9])  \n",
    "\n",
    "test_data = test_data_original.copy() \n",
    "test_data.iloc[:, 1:9] = scaler.fit_transform(test_data_original.iloc[:, 1:9]) \n",
    "\n",
    "train_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate train and test data\n",
    "data_train_test=pd.concat([train_data,test_data])\n",
    "df=data_train_test.iloc[:,1:-1]\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all features\n",
    "\n",
    "all_features = df.iloc[:, :-1]\n",
    "\n",
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train data \n",
    "X_train = train_data.iloc[:,1:-1]\n",
    "\n",
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = train_data['Suscept_3']\n",
    "\n",
    "Y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = test_data.iloc[:,1:-2]\n",
    "\n",
    "X_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_test = test_data['Suscept_3']\n",
    "Y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "print(pd.value_counts(Y_train))\n",
    "\n",
    "print(pd.value_counts(Y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_test_accuracy(f):\n",
    "    print(\"Accuracy = {0}%\".format(100*np.sum(f(X_test) == Y_test)/len(Y_test)))\n",
    "    time.sleep(0.5) # to let the print get out before any progress bars\n",
    "\n",
    "def print_train_accuracy(f):\n",
    "    print(\"Accuracy = {0}%\".format(100*np.sum(f(X_train) == Y_train)/len(Y_train)))\n",
    "    time.sleep(0.5) # to let the print get out before any progress bars\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_features.describe()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model_training"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# nn\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "\n",
    "\n",
    "mlp = MLPClassifier()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    \n",
    "    'hidden_layer_sizes': [(10,), (50,), (100,)],\n",
    "    #'hidden_layer_sizes': [(50,50,50), (50,100,50), (100,)],\n",
    "    'activation': ['relu', 'tanh'],\n",
    "    'solver': ['sgd', 'adam'],\n",
    "    'learning_rate': ['constant', 'adaptive'],\n",
    "    'max_iter': [100, 200, 500],\n",
    "    #'alpha': [0.0001, 0.05],\n",
    "    \n",
    "}\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True,random_state=42\n",
    ")\n",
    "\n",
    "\n",
    "grid_search_mlp = GridSearchCV(mlp, param_grid=param_grid,\n",
    "                           cv=cv, scoring='recall')\n",
    "\n",
    "\n",
    "grid_search_mlp.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print('Best parameters:', grid_search_mlp.best_params_)\n",
    "print('Best AUC:', grid_search_mlp.best_score_)\n",
    "\n",
    "\n",
    "print(\"Best parameters: \", grid_search_mlp.best_params_)\n",
    "print(\"Best score: \", grid_search_mlp.best_score_)\n",
    "\n",
    "\n",
    "\n",
    "params_mlp = grid_search_mlp.best_params_\n",
    "best_params_mlp = MLPClassifier(**params_mlp)\n",
    "best_params_mlp.fit(X_train, Y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# nn\n",
    "\n",
    "model_mlp_new = MLPClassifier( activation= 'tanh', hidden_layer_sizes= (100,), learning_rate= 'constant', max_iter= 500, solver= 'adam')\n",
    "\n",
    "# train model on training set\n",
    "model_mlp_new.fit(X_train, Y_train)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "# LR\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_curve, auc\n",
    "import pandas as pd\n",
    "\n",
    "lr = LogisticRegression()\n",
    "param_grid = {\n",
    "    'penalty': ['l1', 'l2'],\n",
    "    'C': [0.1, 1, 10],\n",
    "    'solver': ['liblinear', 'saga'],\n",
    "    'max_iter': [100, 200, 500],\n",
    "}\n",
    "\n",
    "grid_search_lr = GridSearchCV(lr, param_grid, scoring='recall', cv=cv)\n",
    "grid_search_lr.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid_search_lr.best_params_)\n",
    "print(\"Best score: \", grid_search_lr.best_score_)\n",
    "\n",
    "params_lr = grid_search_lr.best_params_\n",
    "best_params_lr =  LogisticRegression(**params_lr)\n",
    "best_params_lr.fit(X_train, Y_train)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# lr\n",
    "\n",
    "model_lr_new = LogisticRegression(C= 10, max_iter= 100, penalty= 'l1', solver= 'liblinear')\n",
    "\n",
    "model_lr_new.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "rf = RandomForestClassifier()\n",
    "\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 4],\n",
    "}\n",
    "\n",
    "\n",
    "cv = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "grid_search_rf = GridSearchCV(rf, param_grid, scoring='recall', cv=cv)\n",
    "\n",
    "grid_search_rf.fit(X_train, Y_train)\n",
    "\n",
    "print(\"Best parameters: \", grid_search_rf.best_params_)\n",
    "print(\"Best score: \", grid_search_rf.best_score_)\n",
    "\n",
    "params_rf = grid_search_rf.best_params_\n",
    "best_params_rf =  RandomForestClassifier(**params_rf)\n",
    "best_params_rf.fit(X_train, Y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# rf\n",
    "model_rf_new = RandomForestClassifier(max_depth= 20, min_samples_leaf= 1, min_samples_split= 2, n_estimators= 200)\n",
    "\n",
    "# train model on training set\n",
    "model_rf_new.fit(X_train, Y_train)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "\n",
    "xgb_model = xgb.XGBClassifier()\n",
    "    \n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.1, 0.01, 0.001],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'gamma': [0, 0.1, 0.2],\n",
    "    'subsample': [0.8, 0.9, 1],\n",
    "    'colsample_bytree': [0.8, 0.9, 1],\n",
    "}\n",
    "\n",
    "\n",
    "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "grid_search_xgb = GridSearchCV(xgb_model,\n",
    "                           param_grid=param_grid,\n",
    "                           scoring='recall',\n",
    "                           cv=cv)\n",
    "\n",
    "grid_search_xgb.fit(X_train, Y_train)\n",
    "\n",
    "\n",
    "print(\"Best parameters: \", grid_search_xgb.best_params_)\n",
    "print(\"Best score: \", grid_search_xgb.best_score_)\n",
    "\n",
    "\n",
    "params_xgb =grid_search_xgb.best_params_\n",
    "best_params_xgb = xgb.XGBClassifier(**params_xgb)\n",
    "best_params_xgb.fit(X_train, Y_train)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xgb\n",
    "model_xgb_new = xgb.XGBClassifier(colsample_bytree=1, gamma= 0.2, learning_rate= 0.1, max_depth= 7, n_estimators= 200, subsample= 0.9)\n",
    "\n",
    "# train model on training set\n",
    "model_xgb_new.fit(X_train, Y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Model_preformance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score, auc\n",
    "\n",
    "\n",
    "\n",
    "model_lr = model_lr_new\n",
    "model_xgb =  model_xgb_new\n",
    "model_mlp =  model_mlp_new\n",
    "model_rf =  model_rf_new\n",
    "\n",
    "'''\n",
    "\n",
    "model_lr = best_params_lr\n",
    "model_xgb =  best_params_xgb\n",
    "model_mlp =  best_params_mlp\n",
    "model_rf =  best_params_rf\n",
    "'''\n",
    "\n",
    "\n",
    "Y_test_pred1 = model_lr.predict(X_test)\n",
    "Y_test_pred2 = model_xgb.predict(X_test)\n",
    "Y_test_pred3 = model_mlp.predict(X_test)\n",
    "Y_test_pred4 = model_rf.predict(X_test)\n",
    "\n",
    "\n",
    "accuracy_train1 = accuracy_score(Y_train, model_lr.predict(X_train))\n",
    "accuracy_train2 = accuracy_score(Y_train, model_xgb.predict(X_train))\n",
    "accuracy_train3 = accuracy_score(Y_train, model_mlp.predict(X_train))\n",
    "accuracy_train4 = accuracy_score(Y_train, model_rf.predict(X_train))\n",
    "\n",
    "accuracy_test1 = accuracy_score(Y_test, model_lr.predict(X_test))\n",
    "accuracy_test2 = accuracy_score(Y_test, model_xgb.predict(X_test))\n",
    "accuracy_test3 = accuracy_score(Y_test, model_mlp.predict(X_test))\n",
    "accuracy_test4 = accuracy_score(Y_test, model_rf.predict(X_test))\n",
    "\n",
    "precision_train1 = precision_score(Y_train, model_lr.predict(X_train))\n",
    "precision_train2 = precision_score(Y_train, model_xgb.predict(X_train))\n",
    "precision_train3 = precision_score(Y_train, model_mlp.predict(X_train))\n",
    "precision_train4 = precision_score(Y_train, model_rf.predict(X_train))\n",
    "\n",
    "precision_test1 = precision_score(Y_test, Y_test_pred1)\n",
    "precision_test2 = precision_score(Y_test, Y_test_pred2)\n",
    "precision_test3 = precision_score(Y_test, Y_test_pred3)\n",
    "precision_test4 = precision_score(Y_test, Y_test_pred4)\n",
    "\n",
    "recall_train1 = recall_score(Y_train, model_lr.predict(X_train))\n",
    "recall_train2 = recall_score(Y_train, model_xgb.predict(X_train))\n",
    "recall_train3 = recall_score(Y_train, model_mlp.predict(X_train))\n",
    "recall_train4 = recall_score(Y_train, model_rf.predict(X_train))\n",
    "\n",
    "recall_test1 = recall_score(Y_test, Y_test_pred1)\n",
    "recall_test2 = recall_score(Y_test, Y_test_pred2)\n",
    "recall_test3 = recall_score(Y_test, Y_test_pred3)\n",
    "recall_test4 = recall_score(Y_test, Y_test_pred4)\n",
    "\n",
    "f1score_train1 = f1_score(Y_train, model_lr.predict(X_train))\n",
    "f1score_train2 = f1_score(Y_train, model_xgb.predict(X_train))\n",
    "f1score_train3 = f1_score(Y_train, model_mlp.predict(X_train))\n",
    "f1score_train4 = f1_score(Y_train, model_rf.predict(X_train))\n",
    "\n",
    "f1score_test1 = f1_score(Y_test, Y_test_pred1)\n",
    "f1score_test2 = f1_score(Y_test, Y_test_pred2)\n",
    "f1score_test3 = f1_score(Y_test, Y_test_pred3)\n",
    "f1score_test4 = f1_score(Y_test, Y_test_pred4)\n",
    "\n",
    "\n",
    "results_train = {\n",
    "    'Model': ['Model_lr', 'Model_xgb', 'Model_mlp'],\n",
    "    'Accuracy (train)': [accuracy_train1, accuracy_train2, accuracy_train3],\n",
    "    'Precision (train)': [precision_train1, precision_train2, precision_train3],\n",
    "    'Recall (train)': [recall_train1, recall_train2, recall_train3],\n",
    "    'F1 score (train)': [f1score_train1, f1score_train2, f1score_train3],\n",
    "}\n",
    "results_test = {\n",
    "    'Model': ['Model_lr', 'Model_xgb', 'Model_mlp'],\n",
    "    'Accuracy (test)': [accuracy_test1, accuracy_test2, accuracy_test3],\n",
    "    'Precision (test)': [precision_test1, precision_test2, precision_test3],\n",
    "    'Recall (test)': [recall_test1, recall_test2, recall_test3],\n",
    "    'F1 score (test)': [f1score_test1, f1score_test2, f1score_test3],\n",
    "}\n",
    "\n",
    "\n",
    "df_results_train = pd.DataFrame(results_train)\n",
    "df_results_test = pd.DataFrame(results_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "results_train_rf = {\n",
    "    'Model': ['Model_lr', 'Model_xgb', 'Model_mlp','Model_rf'],\n",
    "    'Accuracy (train)': [accuracy_train1, accuracy_train2, accuracy_train3, accuracy_train4],\n",
    "    'Precision (train)': [precision_train1, precision_train2, precision_train3,precision_train4],\n",
    "    'Recall (train)': [recall_train1, recall_train2, recall_train3,recall_train4],\n",
    "    'F1 score (train)': [f1score_train1, f1score_train2, f1score_train3,f1score_train4],\n",
    "}\n",
    "results_test_rf = {\n",
    "    'Model': ['Model_lr', 'Model_xgb', 'Model_mlp','Model_rf'],\n",
    "    'Accuracy (test)': [accuracy_test1, accuracy_test2, accuracy_test3,accuracy_test4],\n",
    "    'Precision (test)': [precision_test1, precision_test2, precision_test3, precision_test4],\n",
    "    'Recall (test)': [recall_test1, recall_test2, recall_test3,recall_test4],\n",
    "    'F1 score (test)': [f1score_test1, f1score_test2, f1score_test3,f1score_test4],\n",
    "}\n",
    "\n",
    "\n",
    "df_results_train_rf = pd.DataFrame(results_train_rf)\n",
    "df_results_test_rf = pd.DataFrame(results_test_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_train_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results_test_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute performance metrics\n",
    "Y_test1 = Y_test.ravel() \n",
    "xgb_loss = sklearn.metrics.log_loss(Y_test1, model_xgb.predict_proba(X_test)[:,1])\n",
    "linear_loss = sklearn.metrics.log_loss(Y_test1, model_lr.predict_proba(X_test)[:,1])\n",
    "nn_loss = sklearn.metrics.log_loss(Y_test1, model_mlp.predict_proba(X_test)[:,1])\n",
    "rf_loss = sklearn.metrics.log_loss(Y_test1, model_rf.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"XGBoost_loss\", xgb_loss)\n",
    "print(\"LR_loss\", linear_loss)\n",
    "print(\"MLP_loss\", nn_loss)\n",
    "print(\"RF_loss\", nn_loss)\n",
    "print()\n",
    "\n",
    "xgb_roc_auc = sklearn.metrics.roc_auc_score(Y_test1, model_xgb.predict_proba(X_test)[:,1])\n",
    "linear_roc_auc = sklearn.metrics.roc_auc_score(Y_test1, model_lr.predict_proba(X_test)[:,1])\n",
    "dnn_roc_auc = sklearn.metrics.roc_auc_score(Y_test1, model_mlp.predict_proba(X_test)[:,1])\n",
    "rf_roc_auc = sklearn.metrics.roc_auc_score(Y_test1, model_rf.predict_proba(X_test)[:,1])\n",
    "\n",
    "print(\"XGBoost_roc_auc\", xgb_roc_auc)\n",
    "print(\"LR_roc_auc\", linear_roc_auc)\n",
    "print(\"MLP_roc_auc\", dnn_roc_auc)\n",
    "print(\"RF_roc_auc\", rf_roc_auc)\n",
    "\n",
    "print()\n",
    "\n",
    "xgb_pr_auc = sklearn.metrics.average_precision_score(Y_test1, model_xgb.predict_proba(X_test)[:,1])\n",
    "linear_pr_auc = sklearn.metrics.average_precision_score(Y_test1, model_lr.predict_proba(X_test)[:,1])\n",
    "dnn_pr_auc = sklearn.metrics.average_precision_score(Y_test1, model_mlp.predict_proba(X_test)[:,1])\n",
    "rf_pr_auc = sklearn.metrics.average_precision_score(Y_test1, model_rf.predict_proba(X_test)[:,1])\n",
    "print(\"XGBoost_pr_auc\", xgb_pr_auc)\n",
    "print(\"LR_pr_auc\", linear_pr_auc)\n",
    "print(\"MLP_pr_auc\", dnn_pr_auc)\n",
    "print(\"RF_pr_auc\", rf_pr_auc)\n",
    "print()\n",
    "\n",
    "xgb_fpr, xgb_tpr, thresholds = sklearn.metrics.roc_curve(Y_test1, model_xgb.predict_proba(X_test)[:,1])\n",
    "linear_fpr, linear_tpr, thresholds = sklearn.metrics.roc_curve(Y_test1, model_lr.predict_proba(X_test)[:,1])\n",
    "dnn_fpr, dnn_tpr, thresholds = sklearn.metrics.roc_curve(Y_test1, model_mlp.predict_proba(X_test)[:,1])\n",
    "rf_fpr, rf_tpr, thresholds = sklearn.metrics.roc_curve(Y_test1, model_rf.predict_proba(X_test)[:,1])\n",
    "\n",
    "\n",
    "\n",
    "pl.plot(rf_fpr, rf_tpr, label=\"AUC RF %.3f\" % rf_roc_auc)\n",
    "pl.plot(dnn_fpr, dnn_tpr, label=\"AUC MLP %.3f\" % dnn_roc_auc)\n",
    "pl.plot(linear_fpr, linear_tpr, label=\"AUC LR %.3f\" % linear_roc_auc)\n",
    "pl.legend()\n",
    "pl.title(\"ROC curves on test data\")\n",
    "pl.show()\n",
    "\n",
    "xgb_prec, xgb_recall, thresholds = sklearn.metrics.precision_recall_curve(Y_test1, model_xgb.predict_proba(X_test)[:,1])\n",
    "dnn_prec, dnn_recall, thresholds = sklearn.metrics.precision_recall_curve(Y_test1, model_mlp.predict_proba(X_test)[:,1])\n",
    "linear_prec, linear_recall, thresholds = sklearn.metrics.precision_recall_curve(Y_test1, model_lr.predict_proba(X_test)[:,1])\n",
    "rf_prec, rf_recall, thresholds = sklearn.metrics.precision_recall_curve(Y_test1, model_rf.predict_proba(X_test)[:,1])\n",
    "\n",
    "\n",
    "pl.plot(dnn_recall, dnn_prec, label=\"MLP %.3f\" % dnn_pr_auc)\n",
    "pl.plot(linear_recall, linear_prec, label=\"LR %.3f\" % linear_pr_auc)\n",
    "pl.plot(rf_recall, rf_prec, label=\"RF %.3f\" % rf_pr_auc)\n",
    "\n",
    "pl.legend()\n",
    "pl.title(\"Precision-Recall curves on test data\")\n",
    "pl.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save output\n",
    "output_test_lr=pd.concat([test_data,pd.Series(model_lr.predict_proba(X_test)[:,1],name='probability_lr')], axis=1)\n",
    "output_test_xgb=pd.concat([test_data,pd.Series(model_xgb.predict_proba(X_test)[:,1],name='probability_xgb')], axis=1)\n",
    "output_test_mlp=pd.concat([test_data,pd.Series(model_mlp.predict_proba(X_test)[:,1],name='probability_mlp')], axis=1)\n",
    "output_test_rf=pd.concat([test_data,pd.Series(model_rf.predict_proba(X_test)[:,1],name='probability_rf')], axis=1)\n",
    "\n",
    "\n",
    "\n",
    "output_test_lr.to_csv('output_test_lr.csv', index=False) \n",
    "output_test_xgb.to_csv('output_test_xgb.csv', index=False) \n",
    "output_test_mlp.to_csv('output_test_mlp.csv', index=False) \n",
    "output_test_rf.to_csv('output_test_rf.csv', index=False) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LIME and SHAP feature contribution "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.reset_index(drop=True)\n",
    "X_test.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SHAP-RF\n",
    "\n",
    "explainer = shap.TreeExplainer(model_rf_new)\n",
    "\n",
    "\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "feature_important_rf = pd.DataFrame(shap_values[1], columns=X_test.columns)\n",
    "\n",
    "feature_important_rf.to_csv('feature_important_shap_rf.csv', index=False)\n",
    "\n",
    "feature_important_rf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# LIME-RF\n",
    "\n",
    "explainer = lime.lime_tabular.LimeTabularExplainer(\n",
    "    training_data=np.array(X_train), \n",
    "    feature_names=X_train.columns.tolist(),  \n",
    "    class_names=['Class 0', 'Class 1'],  \n",
    "    mode='classification'  \n",
    ")\n",
    "\n",
    "str1 = [\"Max_1HR\", \"Max_1HR_2\", \"Max_1HR_72\", \"Max_1TD\", 'RH_Count', 'TD_Count', 'EV', 'TWI', 'DTW']\n",
    "\n",
    "feature_importance_df = pd.DataFrame(columns=str1)\n",
    "\n",
    "for i in range(len(X_test)):\n",
    "    \n",
    "    exp = explainer.explain_instance(\n",
    "        X_test.iloc[i].values, \n",
    "        model_rf_new.predict_proba, \n",
    "        num_features=len(str1)\n",
    "    )\n",
    "    \n",
    "   \n",
    "    feature_importance_scores = {feature: 0 for feature in str1}\n",
    "    for feature, importance in exp.as_list():\n",
    "        for feature_name in str1:\n",
    "            if feature_name in feature: \n",
    "                feature_importance_scores[feature_name] += importance  \n",
    "\n",
    "    \n",
    "    feature_importance_df.loc[len(feature_importance_df)] = feature_importance_scores\n",
    "\n",
    "# save the result\n",
    "feature_importance_df.to_csv('feature_important_lime_rf_240328.csv', index=False)\n",
    "\n",
    "print(feature_importance_df.head())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
