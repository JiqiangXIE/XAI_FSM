{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split_event_based and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取26个筛选后的CSV文件（16868个单元）\n",
    "# 读取26个CSV文件，并存储到df_到df_26变量中\n",
    "# 降雨事件1-16\n",
    "df_1 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df1_240218.csv')\n",
    "df_2 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df2_240218.csv')\n",
    "df_3 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df3_240218.csv')\n",
    "df_4 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df4_240218.csv')\n",
    "df_5 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df5_240218.csv')\n",
    "df_6 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df6_240218.csv')\n",
    "df_7 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df7_240218.csv')\n",
    "df_8 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df8_240218.csv')\n",
    "df_9 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df9_240218.csv')\n",
    "df_10 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df10_240218.csv')\n",
    "df_11 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df11_240218.csv')\n",
    "df_12 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df12_240218.csv')\n",
    "df_13 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df13_240218.csv')\n",
    "df_14 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df14_240218.csv')\n",
    "df_15 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df15_240218.csv')\n",
    "df_16 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df16_240218.csv')\n",
    "df_17 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df17_240218.csv')\n",
    "df_18 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df18_240218.csv')\n",
    "df_19 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df19_240218.csv')\n",
    "df_20 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/df20_240218.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事件重新排序，从大到小，1-9降雨主导，10-18潮汐，20-26降雨-潮汐\n",
    "\n",
    "# 将所有事件放入一个列表中\n",
    "dff_s = [df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10,df_11,df_12,df_13,df_14,df_15,df_16,df_17,df_18,df_19,df_20] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特征工程\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_13308\\2113804859.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_1 has 16868 rows.\n",
      "DataFrame DF_2 has 16868 rows.\n",
      "DataFrame DF_3 has 16868 rows.\n",
      "DataFrame DF_4 has 16868 rows.\n",
      "DataFrame DF_5 has 16868 rows.\n",
      "DataFrame DF_6 has 16868 rows.\n",
      "DataFrame DF_7 has 16868 rows.\n",
      "DataFrame DF_8 has 16868 rows.\n",
      "DataFrame DF_9 has 16868 rows.\n",
      "DataFrame DF_10 has 16868 rows.\n",
      "DataFrame DF_11 has 16868 rows.\n",
      "DataFrame DF_12 has 16868 rows.\n",
      "DataFrame DF_13 has 16868 rows.\n",
      "DataFrame DF_14 has 16868 rows.\n",
      "DataFrame DF_15 has 16868 rows.\n",
      "DataFrame DF_16 has 16868 rows.\n",
      "DataFrame DF_17 has 16868 rows.\n",
      "DataFrame DF_18 has 16868 rows.\n",
      "DataFrame DF_19 has 16868 rows.\n",
      "DataFrame DF_20 has 16868 rows.\n"
     ]
    }
   ],
   "source": [
    "# 特征生成\n",
    "# max_1RH, max_1RH_TD, max_1_2h_R, max_1_72h_R\n",
    "# max_1TD, max_1TD_RH\n",
    "# 降雨超过0mm总小时数,潮水位超过0.8m的总小时数\n",
    "\n",
    "# 现在有26个df，每个df有多个列，包括FID_，DateTime，RH，TD_HR，HR_2，HR_72。每个df有多个FID_，每个FID_对应多个DateTime，每个DateTime对应一个RH，TD_HR，HR_2，HR_72。现在要计算每个df中每个FID_的最大RH，并利用最大RH所在DateTime提取相应的RH, TD_HR，HR_2和HR_72，将结果保存在26个新的df中，并输出到CSV文件中\n",
    "\n",
    "# 假设您的26个DataFrame存储在一个名为df_list的列表中\n",
    "# 创建一个字典来存储结果\n",
    "result_dfs = {}\n",
    "\n",
    "# 遍历每个DataFrame\n",
    "for i, df in enumerate(dff_s):\n",
    "    # 按FID_进行分组\n",
    "    grouped = df.groupby('FID_')\n",
    "    \n",
    "    # 创建一个空的DataFrame来存储当前DataFrame的结果\n",
    "    df_result = pd.DataFrame(columns=['FID_', 'Max_1HR', 'Max_1HR_TD', 'Max_1HR_2', 'Max_1HR_72','Max_1TD','Max_1TD_HR', 'Time_interval','RH_Count', 'TD_Count','EV', 'DTW','TWI', 'Suscept_3'])\n",
    "    \n",
    "    # 遍历每个分组\n",
    "    for fid, group_df in grouped:\n",
    "        # 找到最大的RH和对应的DateTime\n",
    "        max_rh_row = group_df.loc[group_df['RH'].idxmax()]\n",
    "        max_td_row = group_df.loc[group_df['TD_HR'].idxmax()]\n",
    "        \n",
    "        # 提取相应的降雨\n",
    "        max_rh = max_rh_row['RH']\n",
    "        max_rh_datetime = max_rh_row['DateTime']\n",
    "        max_td_hr = max_rh_row['TD_HR']\n",
    "        max_hr_2 = max_rh_row['HR_2']\n",
    "        max_hr_72 = max_rh_row['HR_72']\n",
    "        \n",
    "        # 提取相应的潮汐特征\n",
    "        max_td_datetime = max_td_row['DateTime']\n",
    "        max_td = max_td_row['TD_HR']\n",
    "        max_hr_td = max_td_row['RH']\n",
    "        \n",
    "        # 提取最大降雨和最大潮汐时间间隔\n",
    "        # 计算时间间隔\n",
    "        # 找到最大降雨所在行的行号\n",
    "        max_rain_row = group_df['RH'].idxmax()\n",
    "        \n",
    "        # 找到最大潮汐所在行的行号\n",
    "        max_tide_row = group_df['TD_HR'].idxmax()\n",
    "        \n",
    "        time_interval = abs(max_rain_row - max_tide_row)\n",
    "        \n",
    "        # 降雨超过0mm总小时数,潮水位超过0.8m的总小时数\n",
    "        # 现在有26个df，每个df有多个列，包括FID_，DateTime，RH，TD_HR。每个df有多个FID_，每个FID_对应多个DateTime，每个DateTime对应一个RH，TD_HR。现在要计算每个df中每个FID_的对应的RH的数值大于0的个数，还需要计算每个df中每个FID_的对应的TD_HR的数值大于0.8的个数，将结果保存在26个新的df中，并输出到CSV文件中\n",
    "        # 计算RH大于0的个数\n",
    "        rh_gt_0_count = (group_df['RH'] > 0).sum()\n",
    "        \n",
    "        # 计算TD_HR大于0.8的个数\n",
    "        td_hr_gt_0_8_count = (group_df['TD_HR'] > 0.8).sum()\n",
    "        \n",
    "        # 易涝性标签0.3m\n",
    "        max_w_depth = group_df['w_depth'].max()\n",
    "        \n",
    "        # 将最大w_depth大于0.3的赋值为1，否则赋值为0\n",
    "        max_w_depth_binary = 1 if max_w_depth > 0.3 else 0\n",
    "        \n",
    "        # 地形特征提取\n",
    "        max_ev = max_rh_row['ELV']\n",
    "        max_dtw = max_rh_row['DTW']\n",
    "        max_twi = max_rh_row['TWI']\n",
    "        \n",
    "        \n",
    "        # 将结果添加到df_result，使用pd.concat添加新行\n",
    "        df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh], \n",
    "                                      'Max_1HR_TD': [max_td_hr], 'Max_1HR_2': [max_hr_2], 'Max_1HR_72': [max_hr_72], 'Max_1TD': [max_td], 'Max_1TD_HR': [max_hr_td], 'Time_interval': [time_interval], 'RH_Count': [rh_gt_0_count], 'TD_Count':[td_hr_gt_0_8_count], 'EV':[max_ev], 'DTW':[max_dtw], 'TWI':[max_twi],'Suscept_3':[max_w_depth_binary],})],\n",
    "                              ignore_index=True)\n",
    "    \n",
    "    # 将当前DataFrame的结果添加到字典中，使用DataFrame的名称作为键\n",
    "    result_dfs[f'DF_{i+1}'] = df_result\n",
    "\n",
    "# 保存结果DataFrame为CSV文件\n",
    "for df_name, result_df in result_dfs.items():\n",
    "    result_df.to_csv(f'{df_name}_max_1h_features_240218.csv', index=False)\n",
    "\n",
    "# 统计26个特征表的单元个数\n",
    "# 假设您的26个DataFrame存储在一个名为result_dfs的字典中\n",
    "for df_name, df in result_dfs.items():\n",
    "    # 使用shape属性获取行数和列数\n",
    "    num_rows, num_columns = df.shape\n",
    "    \n",
    "    # 打印每个DataFrame的名称和形状\n",
    "    print(f'DataFrame {df_name} has {num_rows} rows.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "# 读取输入数据集\n",
    "dfff_1 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_1_max_1h_features_240218.csv')\n",
    "dfff_2 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_2_max_1h_features_240218.csv')\n",
    "dfff_3 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_3_max_1h_features_240218.csv')\n",
    "dfff_4 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_4_max_1h_features_240218.csv')\n",
    "dfff_5 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_5_max_1h_features_240218.csv')\n",
    "dfff_6 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_6_max_1h_features_240218.csv')\n",
    "dfff_7 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_7_max_1h_features_240218.csv')\n",
    "dfff_8 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_8_max_1h_features_240218.csv')\n",
    "dfff_9 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_9_max_1h_features_240218.csv')\n",
    "dfff_10 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_10_max_1h_features_240218.csv')\n",
    "dfff_11 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_11_max_1h_features_240218.csv')\n",
    "dfff_12 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_12_max_1h_features_240218.csv')\n",
    "dfff_13 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_13_max_1h_features_240218.csv')\n",
    "dfff_14 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_14_max_1h_features_240218.csv')\n",
    "dfff_15 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_15_max_1h_features_240218.csv')\n",
    "dfff_16 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_16_max_1h_features_240218.csv')\n",
    "dfff_17 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_17_max_1h_features_240218.csv')\n",
    "dfff_18 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_18_max_1h_features_240218.csv')\n",
    "dfff_19 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_19_max_1h_features_240218.csv')\n",
    "dfff_20 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_20_max_1h_features_240218.csv')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID_</th>\n",
       "      <th>Max_1HR</th>\n",
       "      <th>Max_1HR_TD</th>\n",
       "      <th>Max_1HR_2</th>\n",
       "      <th>Max_1HR_72</th>\n",
       "      <th>Max_1TD</th>\n",
       "      <th>Max_1TD_HR</th>\n",
       "      <th>Time_interval</th>\n",
       "      <th>RH_Count</th>\n",
       "      <th>TD_Count</th>\n",
       "      <th>EV</th>\n",
       "      <th>DTW</th>\n",
       "      <th>TWI</th>\n",
       "      <th>Suscept_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.986059</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049686</td>\n",
       "      <td>0.049686</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.449655</td>\n",
       "      <td>2032.268709</td>\n",
       "      <td>7.511351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.974714</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.351578</td>\n",
       "      <td>2289.101516</td>\n",
       "      <td>7.397089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.980567</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049710</td>\n",
       "      <td>0.049710</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.355286</td>\n",
       "      <td>2170.271564</td>\n",
       "      <td>7.548316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.964090</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049789</td>\n",
       "      <td>0.049789</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.489952</td>\n",
       "      <td>2261.629379</td>\n",
       "      <td>8.816202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.969007</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.421316</td>\n",
       "      <td>2262.551560</td>\n",
       "      <td>9.006661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16863</th>\n",
       "      <td>17497</td>\n",
       "      <td>1.130915</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.073035</td>\n",
       "      <td>0.073035</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.471787</td>\n",
       "      <td>995.830006</td>\n",
       "      <td>5.545090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16864</th>\n",
       "      <td>17498</td>\n",
       "      <td>1.132246</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.573513</td>\n",
       "      <td>1024.210310</td>\n",
       "      <td>7.424834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16865</th>\n",
       "      <td>17499</td>\n",
       "      <td>1.130484</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.073379</td>\n",
       "      <td>0.073379</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.150453</td>\n",
       "      <td>890.199884</td>\n",
       "      <td>6.642009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16866</th>\n",
       "      <td>17500</td>\n",
       "      <td>1.130777</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.073583</td>\n",
       "      <td>0.073583</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.170294</td>\n",
       "      <td>786.371904</td>\n",
       "      <td>6.314220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16867</th>\n",
       "      <td>17501</td>\n",
       "      <td>1.089033</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.089802</td>\n",
       "      <td>0.089802</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.928563</td>\n",
       "      <td>2973.308531</td>\n",
       "      <td>4.465188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16868 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FID_   Max_1HR  Max_1HR_TD  Max_1HR_2  Max_1HR_72  Max_1TD  \\\n",
       "0          0  0.986059       0.191   0.049686    0.049686     0.45   \n",
       "1          1  0.974714       0.191   0.049737    0.049737     0.45   \n",
       "2          2  0.980567       0.191   0.049710    0.049710     0.45   \n",
       "3          3  0.964090       0.191   0.049789    0.049789     0.45   \n",
       "4          4  0.969007       0.191   0.049765    0.049765     0.45   \n",
       "...      ...       ...         ...        ...         ...      ...   \n",
       "16863  17497  1.130915       0.191   0.073035    0.073035     0.45   \n",
       "16864  17498  1.132246       0.191   0.072721    0.072721     0.45   \n",
       "16865  17499  1.130484       0.191   0.073379    0.073379     0.45   \n",
       "16866  17500  1.130777       0.191   0.073583    0.073583     0.45   \n",
       "16867  17501  1.089033       0.191   0.089802    0.089802     0.45   \n",
       "\n",
       "       Max_1TD_HR  Time_interval  RH_Count  TD_Count        EV          DTW  \\\n",
       "0        0.000389          33736         4         0  3.449655  2032.268709   \n",
       "1        0.000341          33736         4         0  3.351578  2289.101516   \n",
       "2        0.000366          33736         4         0  3.355286  2170.271564   \n",
       "3        0.000294          33736         4         0  3.489952  2261.629379   \n",
       "4        0.000316          33736         4         0  3.421316  2262.551560   \n",
       "...           ...            ...       ...       ...       ...          ...   \n",
       "16863    0.000622          33736         4         0  2.471787   995.830006   \n",
       "16864    0.000621          33736         4         0  2.573513  1024.210310   \n",
       "16865    0.000625          33736         4         0  2.150453   890.199884   \n",
       "16866    0.000627          33736         4         0  2.170294   786.371904   \n",
       "16867    0.000489          33736         4         0  2.928563  2973.308531   \n",
       "\n",
       "            TWI  Suscept_3  \n",
       "0      7.511351          0  \n",
       "1      7.397089          1  \n",
       "2      7.548316          0  \n",
       "3      8.816202          0  \n",
       "4      9.006661          0  \n",
       "...         ...        ...  \n",
       "16863  5.545090          0  \n",
       "16864  7.424834          0  \n",
       "16865  6.642009          0  \n",
       "16866  6.314220          0  \n",
       "16867  4.465188          0  \n",
       "\n",
       "[16868 rows x 14 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfff_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfff_1: 16868 行\n",
      "dfff_2: 16868 行\n",
      "dfff_3: 16868 行\n",
      "dfff_4: 16868 行\n",
      "dfff_5: 16868 行\n",
      "dfff_6: 16868 行\n",
      "dfff_7: 16868 行\n",
      "dfff_8: 16868 行\n",
      "dfff_9: 16868 行\n",
      "dfff_10: 16868 行\n",
      "dfff_11: 16868 行\n",
      "dfff_12: 16868 行\n",
      "dfff_13: 16868 行\n",
      "dfff_14: 16868 行\n",
      "dfff_15: 16868 行\n",
      "dfff_16: 16868 行\n",
      "dfff_17: 16868 行\n",
      "dfff_18: 16868 行\n",
      "dfff_19: 16868 行\n",
      "dfff_20: 16868 行\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 21):\n",
    "    df_name = f'dfff_{i}'\n",
    "    if df_name in globals():\n",
    "        df = globals()[df_name]\n",
    "        num_rows = len(df)\n",
    "        print(f'{df_name}: {num_rows} 行')\n",
    "    else:\n",
    "        print(f'{df_name} 不存在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dffff_1: 0的数量 = 1056, 1的数量 = 1056\n",
      "dffff_2: 0的数量 = 1684, 1的数量 = 1684\n",
      "dffff_3: 0的数量 = 903, 1的数量 = 903\n",
      "dffff_4: 0的数量 = 2427, 1的数量 = 2427\n",
      "dffff_5: 0的数量 = 850, 1的数量 = 850\n",
      "dffff_6: 0的数量 = 595, 1的数量 = 595\n",
      "dffff_7: 0的数量 = 1569, 1的数量 = 1569\n",
      "dffff_8: 0的数量 = 131, 1的数量 = 131\n",
      "dffff_9: 0的数量 = 144, 1的数量 = 144\n",
      "dffff_10: 0的数量 = 386, 1的数量 = 386\n",
      "dffff_11: 0的数量 = 195, 1的数量 = 195\n",
      "dffff_12: 0的数量 = 75, 1的数量 = 75\n",
      "dffff_13: 0的数量 = 355, 1的数量 = 355\n",
      "dffff_14: 0的数量 = 316, 1的数量 = 316\n",
      "dffff_15: 0的数量 = 867, 1的数量 = 867\n",
      "dffff_16: 0的数量 = 159, 1的数量 = 159\n",
      "dffff_17: 0的数量 = 1513, 1的数量 = 1513\n",
      "dffff_18: 0的数量 = 4130, 1的数量 = 4130\n",
      "dffff_19: 0的数量 = 2384, 1的数量 = 2384\n",
      "dffff_20: 0的数量 = 1313, 1的数量 = 1313\n"
     ]
    }
   ],
   "source": [
    "# 01正负样本均衡（选取与正样本同样数量的负样本）\n",
    "#现在有26个df，命名为dffffff_1到dffffff_26，每个df包含多列，其中包含FID_和Suscept_3列，每个FID_对应一个Suscept_3值，Suscept_3值只有0和1，现在请你找到每个df中Suscept_3列为1的行，并随机选取与Suscept_3列为1的行相同数量的Suscept_3列为0的行，将找到的每个df中Suscept_3列为1的行和每个df随机选取的与Suscept_3列为1的行相同数量的Suscept_3列为0的行共同组成一26个新的df，命名为dfffffff_1到dfffffff_26，并输出为CSV文件，最后计算每个新的df0和1的数量\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 创建一个空的DataFrame列表，用于存放新的DataFrame\n",
    "new_dfs = []\n",
    "\n",
    "# 遍历每个DataFrame\n",
    "for i in range(1, 21):\n",
    "    # 获取DataFrame名称\n",
    "    df_name = globals()[f'dfff_{i}']\n",
    "    \n",
    "    # 找到Suscept_3列值为1的行\n",
    "    df_1 = df_name[df_name['Suscept_3'] == 1]\n",
    "    \n",
    "    # 随机选择相同数量的Suscept_3列值为0的行，固定随机种子，以便每次运行时都会得到相同的结果\n",
    "    random.seed(123)# random.seed(123)是为了保证每次运行时都会得到相同的结果\n",
    "    # 找到Suscept_3列值为0的行\n",
    "    df_0 = df_name[df_name['Suscept_3'] == 0]\n",
    "    random_indices = random.sample(df_0.index.tolist(), len(df_1))\n",
    "    df_0_selected = df_0.loc[random_indices]\n",
    "    \n",
    "    # 将找到的行合并为新的DataFrame\n",
    "    new_df = pd.concat([df_1, df_0_selected])\n",
    "    \n",
    "    # 将新的DataFrame添加到列表中\n",
    "    new_dfs.append(new_df)\n",
    "\n",
    "# 将新的DataFrame列表保存为CSV文件，并计算每个新的DataFrame中0和1的数量\n",
    "for i, new_df in enumerate(new_dfs, start=1):\n",
    "    new_df.to_csv(f'dffff_{i}_240218.csv', index=False)\n",
    "    count_0 = (new_df['Suscept_3'] == 0).sum()\n",
    "    count_1 = (new_df['Suscept_3'] == 1).sum()\n",
    "    print(f'dffff_{i}: 0的数量 = {count_0}, 1的数量 = {count_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "# 读取输入数据集\n",
    "\n",
    "dfffffff_1 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_1_240218.csv')\n",
    "dfffffff_2 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_2_240218.csv')\n",
    "dfffffff_3 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_3_240218.csv')\n",
    "dfffffff_4 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_4_240218.csv')\n",
    "dfffffff_5 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_5_240218.csv')\n",
    "dfffffff_6 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_6_240218.csv')\n",
    "dfffffff_7 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_7_240218.csv')\n",
    "dfffffff_8 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_8_240218.csv')\n",
    "dfffffff_9 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_9_240218.csv')\n",
    "dfffffff_10 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_10_240218.csv')\n",
    "dfffffff_11 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_11_240218.csv')\n",
    "dfffffff_12 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_12_240218.csv')\n",
    "dfffffff_13 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_13_240218.csv')\n",
    "dfffffff_14 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_14_240218.csv')\n",
    "dfffffff_15 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_15_240218.csv')\n",
    "dfffffff_16 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_16_240218.csv')\n",
    "dfffffff_17 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_17_240218.csv')\n",
    "dfffffff_18 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_18_240218.csv')\n",
    "dfffffff_19 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_19_240218.csv')\n",
    "dfffffff_20 = pd.read_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dffff_20_240218.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "# 降雨：124作为训练集，3为测试集，3样本量少\n",
    "# 潮汐：678,5\n",
    "# 降雨-潮汐：91011,12\n",
    "\n",
    "dfffffff_s_train = [dfffffff_1, dfffffff_2, dfffffff_3,  dfffffff_4, dfffffff_5, dfffffff_7,dfffffff_8, dfffffff_9, dfffffff_10, dfffffff_11, dfffffff_13, dfffffff_14, dfffffff_15, dfffffff_16] \n",
    "\n",
    "dfffffff_s_test = [dfffffff_6,  dfffffff_12, dfffffff_17, dfffffff_18,  dfffffff_19, dfffffff_20]\n",
    "\n",
    "# 为测试集中每个df事件添加一列，用于标识该事件\n",
    "# 假设这些dataframes已经存在\n",
    "# dfffffff_2, dfffffff_4, ... dfffffff_26\n",
    "\n",
    "names = [\n",
    "    \"dfffffff_6\", \"dfffffff_12\", \"dfffffff_17\", \"dfffffff_18\", \"dfffffff_19\",\n",
    "    \"dfffffff_20\"\n",
    "]\n",
    "\n",
    "# 需要保留每个样本来自哪个事件\n",
    "for i, df in enumerate(dfffffff_s_test):\n",
    "    df['dataframe_name'] = names[i]\n",
    "\n",
    "\n",
    "\n",
    "train_data = pd.concat(dfffffff_s_train, axis=0, ignore_index=True)\n",
    "test_data = pd.concat(dfffffff_s_test, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID_</th>\n",
       "      <th>Max_1HR</th>\n",
       "      <th>Max_1HR_2</th>\n",
       "      <th>Max_1HR_72</th>\n",
       "      <th>Max_1TD</th>\n",
       "      <th>RH_Count</th>\n",
       "      <th>TD_Count</th>\n",
       "      <th>EV</th>\n",
       "      <th>DTW</th>\n",
       "      <th>TWI</th>\n",
       "      <th>Suscept_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.974714</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.351578</td>\n",
       "      <td>2289.101516</td>\n",
       "      <td>7.397089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>0.990649</td>\n",
       "      <td>0.045291</td>\n",
       "      <td>0.045291</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.953922</td>\n",
       "      <td>817.599531</td>\n",
       "      <td>5.707236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>1.042453</td>\n",
       "      <td>0.049527</td>\n",
       "      <td>0.049527</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.132203</td>\n",
       "      <td>843.498715</td>\n",
       "      <td>6.575277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>1.040824</td>\n",
       "      <td>0.049559</td>\n",
       "      <td>0.049559</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.093620</td>\n",
       "      <td>673.058063</td>\n",
       "      <td>6.745586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127</td>\n",
       "      <td>0.941467</td>\n",
       "      <td>0.049989</td>\n",
       "      <td>0.049989</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.555689</td>\n",
       "      <td>1470.892179</td>\n",
       "      <td>7.038356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22079</th>\n",
       "      <td>6948</td>\n",
       "      <td>0.194422</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001356</td>\n",
       "      <td>1.012</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>1.428083</td>\n",
       "      <td>83.205469</td>\n",
       "      <td>9.107760</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22080</th>\n",
       "      <td>10931</td>\n",
       "      <td>0.151848</td>\n",
       "      <td>0.122546</td>\n",
       "      <td>0.135682</td>\n",
       "      <td>1.012</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2.287966</td>\n",
       "      <td>443.189112</td>\n",
       "      <td>4.973335</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22081</th>\n",
       "      <td>8998</td>\n",
       "      <td>0.325736</td>\n",
       "      <td>0.010761</td>\n",
       "      <td>0.022657</td>\n",
       "      <td>1.012</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2.983380</td>\n",
       "      <td>1383.864365</td>\n",
       "      <td>6.184198</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22082</th>\n",
       "      <td>13562</td>\n",
       "      <td>0.325152</td>\n",
       "      <td>0.039975</td>\n",
       "      <td>0.053611</td>\n",
       "      <td>1.012</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>2.615530</td>\n",
       "      <td>791.892376</td>\n",
       "      <td>4.258863</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22083</th>\n",
       "      <td>1446</td>\n",
       "      <td>0.340175</td>\n",
       "      <td>0.008963</td>\n",
       "      <td>0.030100</td>\n",
       "      <td>1.012</td>\n",
       "      <td>9</td>\n",
       "      <td>7</td>\n",
       "      <td>4.399661</td>\n",
       "      <td>1309.155091</td>\n",
       "      <td>4.995539</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>22084 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FID_   Max_1HR  Max_1HR_2  Max_1HR_72  Max_1TD  RH_Count  TD_Count  \\\n",
       "0          1  0.974714   0.049737    0.049737    0.450         4         0   \n",
       "1         54  0.990649   0.045291    0.045291    0.450         4         0   \n",
       "2        109  1.042453   0.049527    0.049527    0.450         4         0   \n",
       "3        110  1.040824   0.049559    0.049559    0.450         4         0   \n",
       "4        127  0.941467   0.049989    0.049989    0.450         4         0   \n",
       "...      ...       ...        ...         ...      ...       ...       ...   \n",
       "22079   6948  0.194422   0.000000    0.001356    1.012         9         7   \n",
       "22080  10931  0.151848   0.122546    0.135682    1.012         9         7   \n",
       "22081   8998  0.325736   0.010761    0.022657    1.012         9         7   \n",
       "22082  13562  0.325152   0.039975    0.053611    1.012         9         7   \n",
       "22083   1446  0.340175   0.008963    0.030100    1.012         9         7   \n",
       "\n",
       "             EV          DTW       TWI  Suscept_3  \n",
       "0      3.351578  2289.101516  7.397089          1  \n",
       "1      2.953922   817.599531  5.707236          1  \n",
       "2      3.132203   843.498715  6.575277          1  \n",
       "3      3.093620   673.058063  6.745586          1  \n",
       "4      2.555689  1470.892179  7.038356          1  \n",
       "...         ...          ...       ...        ...  \n",
       "22079  1.428083    83.205469  9.107760          0  \n",
       "22080  2.287966   443.189112  4.973335          0  \n",
       "22081  2.983380  1383.864365  6.184198          0  \n",
       "22082  2.615530   791.892376  4.258863          0  \n",
       "22083  4.399661  1309.155091  4.995539          0  \n",
       "\n",
       "[22084 rows x 11 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除Max_1HR_TD列，Max_1TD_HR列，Time_interval列\n",
    "train_data = train_data.drop(['Max_1HR_TD', 'Max_1TD_HR', 'Time_interval'], axis=1)\n",
    "\n",
    "train_data.to_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/train_data_240218.csv', index=False)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID_</th>\n",
       "      <th>Max_1HR</th>\n",
       "      <th>Max_1HR_2</th>\n",
       "      <th>Max_1HR_72</th>\n",
       "      <th>Max_1TD</th>\n",
       "      <th>RH_Count</th>\n",
       "      <th>TD_Count</th>\n",
       "      <th>EV</th>\n",
       "      <th>DTW</th>\n",
       "      <th>TWI</th>\n",
       "      <th>Suscept_3</th>\n",
       "      <th>dataframe_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.352417</td>\n",
       "      <td>0.436326</td>\n",
       "      <td>0.685800</td>\n",
       "      <td>0.390</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3.449655</td>\n",
       "      <td>2032.268709</td>\n",
       "      <td>7.511351</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.355080</td>\n",
       "      <td>0.430869</td>\n",
       "      <td>0.678947</td>\n",
       "      <td>0.390</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3.351578</td>\n",
       "      <td>2289.101516</td>\n",
       "      <td>7.397089</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.353717</td>\n",
       "      <td>0.433655</td>\n",
       "      <td>0.682447</td>\n",
       "      <td>0.390</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3.355286</td>\n",
       "      <td>2170.271564</td>\n",
       "      <td>7.548316</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.356383</td>\n",
       "      <td>0.428212</td>\n",
       "      <td>0.675606</td>\n",
       "      <td>0.390</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3.421316</td>\n",
       "      <td>2262.551560</td>\n",
       "      <td>9.006661</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>0.363621</td>\n",
       "      <td>0.413409</td>\n",
       "      <td>0.656957</td>\n",
       "      <td>0.390</td>\n",
       "      <td>20</td>\n",
       "      <td>0</td>\n",
       "      <td>3.781166</td>\n",
       "      <td>2365.707363</td>\n",
       "      <td>7.267272</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20015</th>\n",
       "      <td>8018</td>\n",
       "      <td>0.521911</td>\n",
       "      <td>0.734471</td>\n",
       "      <td>1.744029</td>\n",
       "      <td>1.114</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>2.600617</td>\n",
       "      <td>1712.331516</td>\n",
       "      <td>6.488986</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20016</th>\n",
       "      <td>12268</td>\n",
       "      <td>0.527696</td>\n",
       "      <td>0.762505</td>\n",
       "      <td>1.817066</td>\n",
       "      <td>1.114</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>3.594648</td>\n",
       "      <td>1306.507172</td>\n",
       "      <td>5.679859</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20017</th>\n",
       "      <td>11022</td>\n",
       "      <td>0.550963</td>\n",
       "      <td>0.706094</td>\n",
       "      <td>1.027979</td>\n",
       "      <td>1.114</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>3.346605</td>\n",
       "      <td>706.392861</td>\n",
       "      <td>4.615225</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20018</th>\n",
       "      <td>4927</td>\n",
       "      <td>0.544548</td>\n",
       "      <td>0.819133</td>\n",
       "      <td>1.855931</td>\n",
       "      <td>1.114</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>3.566776</td>\n",
       "      <td>468.308394</td>\n",
       "      <td>5.986253</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20019</th>\n",
       "      <td>10974</td>\n",
       "      <td>0.512306</td>\n",
       "      <td>0.730703</td>\n",
       "      <td>1.750112</td>\n",
       "      <td>1.114</td>\n",
       "      <td>19</td>\n",
       "      <td>10</td>\n",
       "      <td>2.236899</td>\n",
       "      <td>2599.840278</td>\n",
       "      <td>5.395408</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_20</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20020 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FID_   Max_1HR  Max_1HR_2  Max_1HR_72  Max_1TD  RH_Count  TD_Count  \\\n",
       "0          0  0.352417   0.436326    0.685800    0.390        20         0   \n",
       "1          1  0.355080   0.430869    0.678947    0.390        20         0   \n",
       "2          2  0.353717   0.433655    0.682447    0.390        20         0   \n",
       "3          4  0.356383   0.428212    0.675606    0.390        20         0   \n",
       "4          8  0.363621   0.413409    0.656957    0.390        20         0   \n",
       "...      ...       ...        ...         ...      ...       ...       ...   \n",
       "20015   8018  0.521911   0.734471    1.744029    1.114        19        10   \n",
       "20016  12268  0.527696   0.762505    1.817066    1.114        19        10   \n",
       "20017  11022  0.550963   0.706094    1.027979    1.114        19        10   \n",
       "20018   4927  0.544548   0.819133    1.855931    1.114        19        10   \n",
       "20019  10974  0.512306   0.730703    1.750112    1.114        19        10   \n",
       "\n",
       "             EV          DTW       TWI  Suscept_3 dataframe_name  \n",
       "0      3.449655  2032.268709  7.511351          1     dfffffff_6  \n",
       "1      3.351578  2289.101516  7.397089          1     dfffffff_6  \n",
       "2      3.355286  2170.271564  7.548316          1     dfffffff_6  \n",
       "3      3.421316  2262.551560  9.006661          1     dfffffff_6  \n",
       "4      3.781166  2365.707363  7.267272          1     dfffffff_6  \n",
       "...         ...          ...       ...        ...            ...  \n",
       "20015  2.600617  1712.331516  6.488986          0    dfffffff_20  \n",
       "20016  3.594648  1306.507172  5.679859          0    dfffffff_20  \n",
       "20017  3.346605   706.392861  4.615225          0    dfffffff_20  \n",
       "20018  3.566776   468.308394  5.986253          0    dfffffff_20  \n",
       "20019  2.236899  2599.840278  5.395408          0    dfffffff_20  \n",
       "\n",
       "[20020 rows x 12 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除Max_1HR_TD列，Max_1TD_HR列，Time_interval列\n",
    "test_data = test_data.drop(['Max_1HR_TD', 'Max_1TD_HR', 'Time_interval'], axis=1)\n",
    "\n",
    "test_data.to_csv('D:/0. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/test_data_240218.csv', index=False)\n",
    "\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
