{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train_test_split_event_based and feature engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取26个筛选后的CSV文件（16868个单元）\n",
    "# 读取26个CSV文件，并存储到df_到df_26变量中\n",
    "# 降雨事件1-16\n",
    "df_1 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_1_231017_事件2分类.csv')\n",
    "df_2 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_2_231017_事件2分类.csv')\n",
    "df_3 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_3_231017_事件2分类.csv')\n",
    "df_4 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_4_231017_事件2分类.csv')\n",
    "df_5 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_5_231017_事件2分类.csv')\n",
    "df_6 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_6_231017_事件2分类.csv')\n",
    "df_7 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_7_231017_事件2分类.csv')\n",
    "df_8 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_8_231017_事件2分类.csv')\n",
    "df_9 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_9_231017_事件2分类.csv')\n",
    "df_10 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_10_231017_事件2分类.csv')\n",
    "df_11 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_11_231017_事件2分类.csv')\n",
    "df_12 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_12_231017_事件2分类.csv')\n",
    "df_13 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_13_231017_事件2分类.csv')\n",
    "df_14 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_14_231017_事件2分类.csv')\n",
    "df_15 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_15_231017_事件2分类.csv')\n",
    "df_16 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_16_231017_事件2分类.csv')\n",
    "\n",
    "# 潮汐事件17-26\n",
    "df_17 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_17_231017_事件2分类.csv')\n",
    "df_18 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_18_231017_事件2分类.csv')\n",
    "df_19 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_19_231017_事件2分类.csv')\n",
    "df_20 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_20_231017_事件2分类.csv')\n",
    "df_21 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_21_231017_事件2分类.csv')\n",
    "df_22 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_22_231017_事件2分类.csv')\n",
    "df_23 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_23_231017_事件2分类.csv')\n",
    "df_24 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_24_231017_事件2分类.csv')\n",
    "df_25 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_25_231017_事件2分类.csv')\n",
    "df_26 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfn_26_231017_事件2分类.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 事件重新排序，从大到小，1-9降雨主导，10-18潮汐，20-26降雨-潮汐\n",
    "\n",
    "# 将所有事件放入一个列表中\n",
    "dff_s = [df_1, df_2, df_3, df_4, df_5, df_6, df_7, df_8, df_9, df_10,df_11,df_12, df_13, df_14, df_15, df_16, df_17, df_18, df_19, df_20, df_21, df_22, df_23, df_24, df_25, df_26] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n",
      "C:\\Users\\纯\\AppData\\Local\\Temp\\ipykernel_20452\\542286436.py:69: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh],\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrame DF_1 has 16868 rows.\n",
      "DataFrame DF_2 has 16868 rows.\n",
      "DataFrame DF_3 has 16868 rows.\n",
      "DataFrame DF_4 has 16868 rows.\n",
      "DataFrame DF_5 has 16868 rows.\n",
      "DataFrame DF_6 has 16868 rows.\n",
      "DataFrame DF_7 has 16868 rows.\n",
      "DataFrame DF_8 has 16868 rows.\n",
      "DataFrame DF_9 has 16868 rows.\n",
      "DataFrame DF_10 has 16868 rows.\n",
      "DataFrame DF_11 has 16868 rows.\n",
      "DataFrame DF_12 has 16868 rows.\n",
      "DataFrame DF_13 has 16868 rows.\n",
      "DataFrame DF_14 has 16868 rows.\n",
      "DataFrame DF_15 has 16868 rows.\n",
      "DataFrame DF_16 has 16868 rows.\n",
      "DataFrame DF_17 has 16868 rows.\n",
      "DataFrame DF_18 has 16868 rows.\n",
      "DataFrame DF_19 has 16868 rows.\n",
      "DataFrame DF_20 has 16868 rows.\n",
      "DataFrame DF_21 has 16868 rows.\n",
      "DataFrame DF_22 has 16868 rows.\n",
      "DataFrame DF_23 has 16868 rows.\n",
      "DataFrame DF_24 has 16868 rows.\n",
      "DataFrame DF_25 has 16868 rows.\n",
      "DataFrame DF_26 has 16868 rows.\n"
     ]
    }
   ],
   "source": [
    "# 特征生成\n",
    "# max_1RH, max_1RH_TD, max_1_2h_R, max_1_72h_R\n",
    "# max_1TD, max_1TD_RH\n",
    "# 降雨超过0mm总小时数,潮水位超过0.8m的总小时数\n",
    "\n",
    "# 现在有26个df，每个df有多个列，包括FID_，DateTime，RH，TD_HR，HR_2，HR_72。每个df有多个FID_，每个FID_对应多个DateTime，每个DateTime对应一个RH，TD_HR，HR_2，HR_72。现在要计算每个df中每个FID_的最大RH，并利用最大RH所在DateTime提取相应的RH, TD_HR，HR_2和HR_72，将结果保存在26个新的df中，并输出到CSV文件中\n",
    "\n",
    "# 假设您的26个DataFrame存储在一个名为df_list的列表中\n",
    "# 创建一个字典来存储结果\n",
    "result_dfs = {}\n",
    "\n",
    "# 遍历每个DataFrame\n",
    "for i, df in enumerate(dff_s):\n",
    "    # 按FID_进行分组\n",
    "    grouped = df.groupby('FID_')\n",
    "    \n",
    "    # 创建一个空的DataFrame来存储当前DataFrame的结果\n",
    "    df_result = pd.DataFrame(columns=['FID_', 'Max_1HR', 'Max_1HR_TD', 'Max_1HR_2', 'Max_1HR_72','Max_1TD','Max_1TD_HR', 'Time_interval','RH_Count', 'TD_Count','EV', 'DTW','TWI', 'Suscept_3'])\n",
    "    \n",
    "    # 遍历每个分组\n",
    "    for fid, group_df in grouped:\n",
    "        # 找到最大的RH和对应的DateTime\n",
    "        max_rh_row = group_df.loc[group_df['RH'].idxmax()]\n",
    "        max_td_row = group_df.loc[group_df['TD_HR'].idxmax()]\n",
    "        \n",
    "        # 提取相应的降雨\n",
    "        max_rh = max_rh_row['RH']\n",
    "        max_rh_datetime = max_rh_row['DateTime']\n",
    "        max_td_hr = max_rh_row['TD_HR']\n",
    "        max_hr_2 = max_rh_row['HR_2']\n",
    "        max_hr_72 = max_rh_row['HR_72']\n",
    "        \n",
    "        # 提取相应的潮汐特征\n",
    "        max_td_datetime = max_td_row['DateTime']\n",
    "        max_td = max_td_row['TD_HR']\n",
    "        max_hr_td = max_td_row['RH']\n",
    "        \n",
    "        # 提取最大降雨和最大潮汐时间间隔\n",
    "        # 计算时间间隔\n",
    "        # 找到最大降雨所在行的行号\n",
    "        max_rain_row = group_df['RH'].idxmax()\n",
    "        \n",
    "        # 找到最大潮汐所在行的行号\n",
    "        max_tide_row = group_df['TD_HR'].idxmax()\n",
    "        \n",
    "        time_interval = abs(max_rain_row - max_tide_row)\n",
    "        \n",
    "        # 降雨超过0mm总小时数,潮水位超过0.8m的总小时数\n",
    "        # 现在有26个df，每个df有多个列，包括FID_，DateTime，RH，TD_HR。每个df有多个FID_，每个FID_对应多个DateTime，每个DateTime对应一个RH，TD_HR。现在要计算每个df中每个FID_的对应的RH的数值大于0的个数，还需要计算每个df中每个FID_的对应的TD_HR的数值大于0.8的个数，将结果保存在26个新的df中，并输出到CSV文件中\n",
    "        # 计算RH大于0的个数\n",
    "        rh_gt_0_count = (group_df['RH'] > 0).sum()\n",
    "        \n",
    "        # 计算TD_HR大于0.8的个数\n",
    "        td_hr_gt_0_8_count = (group_df['TD_HR'] > 0.8).sum()\n",
    "        \n",
    "        # 易涝性标签0.3m\n",
    "        max_w_depth = group_df['w_depth'].max()\n",
    "        \n",
    "        # 将最大w_depth大于0.3的赋值为1，否则赋值为0\n",
    "        max_w_depth_binary = 1 if max_w_depth > 0.3 else 0\n",
    "        \n",
    "        # 地形特征提取\n",
    "        max_ev = max_rh_row['ELV']\n",
    "        max_dtw = max_rh_row['DTW']\n",
    "        max_twi = max_rh_row['TWI']\n",
    "        \n",
    "        \n",
    "        # 将结果添加到df_result，使用pd.concat添加新行\n",
    "        df_result = pd.concat([df_result, pd.DataFrame({'FID_': [fid], 'Max_1HR': [max_rh], \n",
    "                                      'Max_1HR_TD': [max_td_hr], 'Max_1HR_2': [max_hr_2], 'Max_1HR_72': [max_hr_72], 'Max_1TD': [max_td], 'Max_1TD_HR': [max_hr_td], 'Time_interval': [time_interval], 'RH_Count': [rh_gt_0_count], 'TD_Count':[td_hr_gt_0_8_count], 'EV':[max_ev], 'DTW':[max_dtw], 'TWI':[max_twi],'Suscept_3':[max_w_depth_binary],})],\n",
    "                              ignore_index=True)\n",
    "    \n",
    "    # 将当前DataFrame的结果添加到字典中，使用DataFrame的名称作为键\n",
    "    result_dfs[f'DF_{i+1}'] = df_result\n",
    "\n",
    "# 保存结果DataFrame为CSV文件\n",
    "for df_name, result_df in result_dfs.items():\n",
    "    result_df.to_csv(f'{df_name}_max_1h_features_231017_两类事件.csv', index=False)\n",
    "\n",
    "# 统计26个特征表的单元个数\n",
    "# 假设您的26个DataFrame存储在一个名为result_dfs的字典中\n",
    "for df_name, df in result_dfs.items():\n",
    "    # 使用shape属性获取行数和列数\n",
    "    num_rows, num_columns = df.shape\n",
    "    \n",
    "    # 打印每个DataFrame的名称和形状\n",
    "    print(f'DataFrame {df_name} has {num_rows} rows.')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "属性表已成功导出到CSV文件: path_to_output_csv_file.csv\n"
     ]
    }
   ],
   "source": [
    "# 添加坐标\n",
    "# 从SHP文件中提取属性表数据并将其写入CSV文件\n",
    "import geopandas as gpd\n",
    "\n",
    "# 输入SHP文件路径\n",
    "shp_file = \"D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/数据/原始数据/Street_centerline_50m_buf_prj/Street_centerline_50m_buf_prj.shp\"\n",
    "\n",
    "# 输出CSV文件路径\n",
    "csv_file = \"path_to_output_csv_file.csv\"\n",
    "\n",
    "# 读取SHP文件\n",
    "gdf = gpd.read_file(shp_file)\n",
    "\n",
    "# 将属性表导出为CSV文件\n",
    "gdf.to_csv(csv_file, index=False)\n",
    "\n",
    "print(f\"属性表已成功导出到CSV文件: {csv_file}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "# 读取输入数据集\n",
    "dfff_1 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_1_max_1h_features_231017_两类事件.csv')\n",
    "dfff_2 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_2_max_1h_features_231017_两类事件.csv')\n",
    "dfff_3 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_3_max_1h_features_231017_两类事件.csv')\n",
    "dfff_4 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_4_max_1h_features_231017_两类事件.csv')\n",
    "dfff_5 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_5_max_1h_features_231017_两类事件.csv')\n",
    "dfff_6 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_6_max_1h_features_231017_两类事件.csv')\n",
    "dfff_7 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_7_max_1h_features_231017_两类事件.csv')\n",
    "dfff_8 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_8_max_1h_features_231017_两类事件.csv')\n",
    "dfff_9 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_9_max_1h_features_231017_两类事件.csv')\n",
    "dfff_10 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_10_max_1h_features_231017_两类事件.csv')\n",
    "dfff_11 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_11_max_1h_features_231017_两类事件.csv')\n",
    "dfff_12 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_12_max_1h_features_231017_两类事件.csv')\n",
    "dfff_13 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_13_max_1h_features_231017_两类事件.csv')\n",
    "dfff_14 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_14_max_1h_features_231017_两类事件.csv')\n",
    "dfff_15 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_15_max_1h_features_231017_两类事件.csv')\n",
    "dfff_16 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_16_max_1h_features_231017_两类事件.csv')\n",
    "dfff_17 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_17_max_1h_features_231017_两类事件.csv')\n",
    "dfff_18 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_18_max_1h_features_231017_两类事件.csv')\n",
    "dfff_19 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_19_max_1h_features_231017_两类事件.csv')\n",
    "dfff_20 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_20_max_1h_features_231017_两类事件.csv')\n",
    "dfff_21 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_21_max_1h_features_231017_两类事件.csv')\n",
    "dfff_22 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_22_max_1h_features_231017_两类事件.csv')\n",
    "dfff_23 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_23_max_1h_features_231017_两类事件.csv')\n",
    "dfff_24 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_24_max_1h_features_231017_两类事件.csv')\n",
    "dfff_25 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_25_max_1h_features_231017_两类事件.csv')\n",
    "dfff_26 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/DF_26_max_1h_features_231017_两类事件.csv')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID_</th>\n",
       "      <th>Max_1HR</th>\n",
       "      <th>Max_1HR_TD</th>\n",
       "      <th>Max_1HR_2</th>\n",
       "      <th>Max_1HR_72</th>\n",
       "      <th>Max_1TD</th>\n",
       "      <th>Max_1TD_HR</th>\n",
       "      <th>Time_interval</th>\n",
       "      <th>RH_Count</th>\n",
       "      <th>TD_Count</th>\n",
       "      <th>EV</th>\n",
       "      <th>DTW</th>\n",
       "      <th>TWI</th>\n",
       "      <th>Suscept_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0.986059</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049686</td>\n",
       "      <td>0.049686</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000389</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.449655</td>\n",
       "      <td>2032.268709</td>\n",
       "      <td>7.511351</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>0.974714</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.351578</td>\n",
       "      <td>2289.101516</td>\n",
       "      <td>7.397089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>0.980567</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049710</td>\n",
       "      <td>0.049710</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000366</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.355286</td>\n",
       "      <td>2170.271564</td>\n",
       "      <td>7.548316</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>0.964090</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049789</td>\n",
       "      <td>0.049789</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000294</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.489952</td>\n",
       "      <td>2261.629379</td>\n",
       "      <td>8.816202</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>0.969007</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.049765</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000316</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.421316</td>\n",
       "      <td>2262.551560</td>\n",
       "      <td>9.006661</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16863</th>\n",
       "      <td>17497</td>\n",
       "      <td>1.130915</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.073035</td>\n",
       "      <td>0.073035</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.471787</td>\n",
       "      <td>995.830006</td>\n",
       "      <td>5.545090</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16864</th>\n",
       "      <td>17498</td>\n",
       "      <td>1.132246</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>0.072721</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000621</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.573513</td>\n",
       "      <td>1024.210310</td>\n",
       "      <td>7.424834</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16865</th>\n",
       "      <td>17499</td>\n",
       "      <td>1.130484</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.073379</td>\n",
       "      <td>0.073379</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000625</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.150453</td>\n",
       "      <td>890.199884</td>\n",
       "      <td>6.642009</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16866</th>\n",
       "      <td>17500</td>\n",
       "      <td>1.130777</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.073583</td>\n",
       "      <td>0.073583</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000627</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.170294</td>\n",
       "      <td>786.371904</td>\n",
       "      <td>6.314220</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16867</th>\n",
       "      <td>17501</td>\n",
       "      <td>1.089033</td>\n",
       "      <td>0.191</td>\n",
       "      <td>0.089802</td>\n",
       "      <td>0.089802</td>\n",
       "      <td>0.45</td>\n",
       "      <td>0.000489</td>\n",
       "      <td>33736</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.928563</td>\n",
       "      <td>2973.308531</td>\n",
       "      <td>4.465188</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>16868 rows × 14 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FID_   Max_1HR  Max_1HR_TD  Max_1HR_2  Max_1HR_72  Max_1TD  \\\n",
       "0          0  0.986059       0.191   0.049686    0.049686     0.45   \n",
       "1          1  0.974714       0.191   0.049737    0.049737     0.45   \n",
       "2          2  0.980567       0.191   0.049710    0.049710     0.45   \n",
       "3          3  0.964090       0.191   0.049789    0.049789     0.45   \n",
       "4          4  0.969007       0.191   0.049765    0.049765     0.45   \n",
       "...      ...       ...         ...        ...         ...      ...   \n",
       "16863  17497  1.130915       0.191   0.073035    0.073035     0.45   \n",
       "16864  17498  1.132246       0.191   0.072721    0.072721     0.45   \n",
       "16865  17499  1.130484       0.191   0.073379    0.073379     0.45   \n",
       "16866  17500  1.130777       0.191   0.073583    0.073583     0.45   \n",
       "16867  17501  1.089033       0.191   0.089802    0.089802     0.45   \n",
       "\n",
       "       Max_1TD_HR  Time_interval  RH_Count  TD_Count        EV          DTW  \\\n",
       "0        0.000389          33736         4         0  3.449655  2032.268709   \n",
       "1        0.000341          33736         4         0  3.351578  2289.101516   \n",
       "2        0.000366          33736         4         0  3.355286  2170.271564   \n",
       "3        0.000294          33736         4         0  3.489952  2261.629379   \n",
       "4        0.000316          33736         4         0  3.421316  2262.551560   \n",
       "...           ...            ...       ...       ...       ...          ...   \n",
       "16863    0.000622          33736         4         0  2.471787   995.830006   \n",
       "16864    0.000621          33736         4         0  2.573513  1024.210310   \n",
       "16865    0.000625          33736         4         0  2.150453   890.199884   \n",
       "16866    0.000627          33736         4         0  2.170294   786.371904   \n",
       "16867    0.000489          33736         4         0  2.928563  2973.308531   \n",
       "\n",
       "            TWI  Suscept_3  \n",
       "0      7.511351          0  \n",
       "1      7.397089          1  \n",
       "2      7.548316          0  \n",
       "3      8.816202          0  \n",
       "4      9.006661          0  \n",
       "...         ...        ...  \n",
       "16863  5.545090          0  \n",
       "16864  7.424834          0  \n",
       "16865  6.642009          0  \n",
       "16866  6.314220          0  \n",
       "16867  4.465188          0  \n",
       "\n",
       "[16868 rows x 14 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfff_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfff_1: 16868 行\n",
      "dfff_2: 16868 行\n",
      "dfff_3: 16868 行\n",
      "dfff_4: 16868 行\n",
      "dfff_5: 16868 行\n",
      "dfff_6: 16868 行\n",
      "dfff_7: 16868 行\n",
      "dfff_8: 16868 行\n",
      "dfff_9: 16868 行\n",
      "dfff_10: 16868 行\n",
      "dfff_11: 16868 行\n",
      "dfff_12: 16868 行\n",
      "dfff_13: 16868 行\n",
      "dfff_14: 16868 行\n",
      "dfff_15: 16868 行\n",
      "dfff_16: 16868 行\n",
      "dfff_17: 16868 行\n",
      "dfff_18: 16868 行\n",
      "dfff_19: 16868 行\n",
      "dfff_20: 16868 行\n",
      "dfff_21: 16868 行\n",
      "dfff_22: 16868 行\n",
      "dfff_23: 16868 行\n",
      "dfff_24: 16868 行\n",
      "dfff_25: 16868 行\n",
      "dfff_26: 16868 行\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 27):\n",
    "    df_name = f'dfff_{i}'\n",
    "    if df_name in globals():\n",
    "        df = globals()[df_name]\n",
    "        num_rows = len(df)\n",
    "        print(f'{df_name}: {num_rows} 行')\n",
    "    else:\n",
    "        print(f'{df_name} 不存在')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n# 读取CSV文件为一个DataFrame\\npath_to_output_csv_file = 'path_to_output_csv_file.csv'\\ncsv_df = pd.read_csv(path_to_output_csv_file)\\n\\n# 遍历26个DataFrame，将X和Y值添加到每个DataFrame中\\nfor i in range(1, 27):\\n    df_name = f'dfff_{i}'\\n    if df_name in globals():\\n        df = globals()[df_name]\\n        # 使用FID_列的值与CSV文件中的DataFrame关联\\n        df = df.merge(csv_df[['FID_', 'X_mid', 'Y_mid']], on='FID_', how='left')\\n        \\n        # 将更新后的DataFrame保存为新的CSV文件\\n        new_filename = f'dffff_{i}.csv'\\n        df.to_csv(new_filename, index=False)\\n        \""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 添加坐标\n",
    "\n",
    "'''\n",
    "# 读取CSV文件为一个DataFrame\n",
    "path_to_output_csv_file = 'path_to_output_csv_file.csv'\n",
    "csv_df = pd.read_csv(path_to_output_csv_file)\n",
    "\n",
    "# 遍历26个DataFrame，将X和Y值添加到每个DataFrame中\n",
    "for i in range(1, 27):\n",
    "    df_name = f'dfff_{i}'\n",
    "    if df_name in globals():\n",
    "        df = globals()[df_name]\n",
    "        # 使用FID_列的值与CSV文件中的DataFrame关联\n",
    "        df = df.merge(csv_df[['FID_', 'X_mid', 'Y_mid']], on='FID_', how='left')\n",
    "        \n",
    "        # 将更新后的DataFrame保存为新的CSV文件\n",
    "        new_filename = f'dffff_{i}.csv'\n",
    "        df.to_csv(new_filename, index=False)\n",
    "        '''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dfffffff_1: 0的数量 = 1056, 1的数量 = 1056\n",
      "dfffffff_2: 0的数量 = 4130, 1的数量 = 4130\n",
      "dfffffff_3: 0的数量 = 2743, 1的数量 = 2743\n",
      "dfffffff_4: 0的数量 = 903, 1的数量 = 903\n",
      "dfffffff_5: 0的数量 = 1513, 1的数量 = 1513\n",
      "dfffffff_6: 0的数量 = 2384, 1的数量 = 2384\n",
      "dfffffff_7: 0的数量 = 1684, 1的数量 = 1684\n",
      "dfffffff_8: 0的数量 = 2427, 1的数量 = 2427\n",
      "dfffffff_9: 0的数量 = 1044, 1的数量 = 1044\n",
      "dfffffff_10: 0的数量 = 850, 1的数量 = 850\n",
      "dfffffff_11: 0的数量 = 1569, 1的数量 = 1569\n",
      "dfffffff_12: 0的数量 = 595, 1的数量 = 595\n",
      "dfffffff_13: 0的数量 = 1619, 1的数量 = 1619\n",
      "dfffffff_14: 0的数量 = 649, 1的数量 = 649\n",
      "dfffffff_15: 0的数量 = 1316, 1的数量 = 1316\n",
      "dfffffff_16: 0的数量 = 227, 1的数量 = 227\n",
      "dfffffff_17: 0的数量 = 1313, 1的数量 = 1313\n",
      "dfffffff_18: 0的数量 = 195, 1的数量 = 195\n",
      "dfffffff_19: 0的数量 = 867, 1的数量 = 867\n",
      "dfffffff_20: 0的数量 = 355, 1的数量 = 355\n",
      "dfffffff_21: 0的数量 = 386, 1的数量 = 386\n",
      "dfffffff_22: 0的数量 = 159, 1的数量 = 159\n",
      "dfffffff_23: 0的数量 = 131, 1的数量 = 131\n",
      "dfffffff_24: 0的数量 = 144, 1的数量 = 144\n",
      "dfffffff_25: 0的数量 = 316, 1的数量 = 316\n",
      "dfffffff_26: 0的数量 = 75, 1的数量 = 75\n"
     ]
    }
   ],
   "source": [
    "# 01正负样本均衡（选取与正样本同样数量的负样本）\n",
    "#现在有26个df，命名为dffffff_1到dffffff_26，每个df包含多列，其中包含FID_和Suscept_3列，每个FID_对应一个Suscept_3值，Suscept_3值只有0和1，现在请你找到每个df中Suscept_3列为1的行，并随机选取与Suscept_3列为1的行相同数量的Suscept_3列为0的行，将找到的每个df中Suscept_3列为1的行和每个df随机选取的与Suscept_3列为1的行相同数量的Suscept_3列为0的行共同组成一26个新的df，命名为dfffffff_1到dfffffff_26，并输出为CSV文件，最后计算每个新的df0和1的数量\n",
    "\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "# 创建一个空的DataFrame列表，用于存放新的DataFrame\n",
    "new_dfs = []\n",
    "\n",
    "# 遍历每个DataFrame\n",
    "for i in range(1, 27):\n",
    "    # 获取DataFrame名称\n",
    "    df_name = globals()[f'dfff_{i}']\n",
    "    \n",
    "    # 找到Suscept_3列值为1的行\n",
    "    df_1 = df_name[df_name['Suscept_3'] == 1]\n",
    "    \n",
    "    # 随机选择相同数量的Suscept_3列值为0的行，固定随机种子，以便每次运行时都会得到相同的结果\n",
    "    random.seed(123)# random.seed(123)是为了保证每次运行时都会得到相同的结果\n",
    "    # 找到Suscept_3列值为0的行\n",
    "    df_0 = df_name[df_name['Suscept_3'] == 0]\n",
    "    random_indices = random.sample(df_0.index.tolist(), len(df_1))\n",
    "    df_0_selected = df_0.loc[random_indices]\n",
    "    \n",
    "    # 将找到的行合并为新的DataFrame\n",
    "    new_df = pd.concat([df_1, df_0_selected])\n",
    "    \n",
    "    # 将新的DataFrame添加到列表中\n",
    "    new_dfs.append(new_df)\n",
    "\n",
    "# 将新的DataFrame列表保存为CSV文件，并计算每个新的DataFrame中0和1的数量\n",
    "for i, new_df in enumerate(new_dfs, start=1):\n",
    "    new_df.to_csv(f'dfffffff_{i}_231017_两类事件.csv', index=False)\n",
    "    count_0 = (new_df['Suscept_3'] == 0).sum()\n",
    "    count_1 = (new_df['Suscept_3'] == 1).sum()\n",
    "    print(f'dfffffff_{i}: 0的数量 = {count_0}, 1的数量 = {count_1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "# 读取输入数据集\n",
    "\n",
    "dfffffff_1 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_1_231017_两类事件.csv')\n",
    "dfffffff_2 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_2_231017_两类事件.csv')\n",
    "dfffffff_3 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_3_231017_两类事件.csv')\n",
    "dfffffff_4 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_4_231017_两类事件.csv')\n",
    "dfffffff_5 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_5_231017_两类事件.csv')\n",
    "dfffffff_6 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_6_231017_两类事件.csv')\n",
    "dfffffff_7 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_7_231017_两类事件.csv')\n",
    "dfffffff_8 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_8_231017_两类事件.csv')\n",
    "dfffffff_9 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_9_231017_两类事件.csv')\n",
    "dfffffff_10 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_10_231017_两类事件.csv')\n",
    "dfffffff_11 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_11_231017_两类事件.csv')\n",
    "dfffffff_12 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_12_231017_两类事件.csv')\n",
    "dfffffff_13 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_13_231017_两类事件.csv')\n",
    "dfffffff_14 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_14_231017_两类事件.csv')\n",
    "dfffffff_15 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_15_231017_两类事件.csv')\n",
    "dfffffff_16 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_16_231017_两类事件.csv')\n",
    "dfffffff_17 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_17_231017_两类事件.csv')\n",
    "dfffffff_18 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_18_231017_两类事件.csv')\n",
    "dfffffff_19 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_19_231017_两类事件.csv')\n",
    "dfffffff_20 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_20_231017_两类事件.csv')\n",
    "dfffffff_21 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_21_231017_两类事件.csv')\n",
    "dfffffff_22 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_22_231017_两类事件.csv')\n",
    "dfffffff_23 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_23_231017_两类事件.csv')\n",
    "dfffffff_24 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_24_231017_两类事件.csv')\n",
    "dfffffff_25= pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_25_231017_两类事件.csv')\n",
    "dfffffff_26 = pd.read_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/dfffffff_26_231017_两类事件.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 划分训练集和测试集\n",
    "# 降雨、潮汐、降雨-潮汐事件（1-13降雨，14-19潮汐，20-26降雨-潮汐\n",
    "# 231011 降雨1-13，潮汐14-22，R-T：23-26\n",
    "# 间隔取样\n",
    "\n",
    "dfffffff_s_train = [dfffffff_1, dfffffff_3, dfffffff_5,  dfffffff_7, dfffffff_9, dfffffff_11, dfffffff_13, dfffffff_15,  dfffffff_17, dfffffff_19, dfffffff_21, dfffffff_23,  dfffffff_25] \n",
    "\n",
    "dfffffff_s_test = [dfffffff_2,  dfffffff_4, dfffffff_6,dfffffff_8,  dfffffff_10,dfffffff_12, dfffffff_14,  dfffffff_16,  dfffffff_18,dfffffff_20,  dfffffff_22, dfffffff_24, dfffffff_26]\n",
    "\n",
    "# 为测试集中每个df事件添加一列，用于标识该事件\n",
    "# 假设这些dataframes已经存在\n",
    "# dfffffff_2, dfffffff_4, ... dfffffff_26\n",
    "\n",
    "names = [\n",
    "    \"dfffffff_2\", \"dfffffff_4\", \"dfffffff_6\", \"dfffffff_8\", \"dfffffff_10\",\n",
    "    \"dfffffff_12\", \"dfffffff_14\", \"dfffffff_16\", \"dfffffff_18\", \"dfffffff_20\",\n",
    "    \"dfffffff_22\", \"dfffffff_24\", \"dfffffff_26\"\n",
    "]\n",
    "\n",
    "# 需要保留每个样本来自哪个事件\n",
    "for i, df in enumerate(dfffffff_s_test):\n",
    "    df['dataframe_name'] = names[i]\n",
    "\n",
    "\n",
    "\n",
    "train_data = pd.concat(dfffffff_s_train, axis=0, ignore_index=True)\n",
    "test_data = pd.concat(dfffffff_s_test, axis=0, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID_</th>\n",
       "      <th>Max_1HR</th>\n",
       "      <th>Max_1HR_2</th>\n",
       "      <th>Max_1HR_72</th>\n",
       "      <th>Max_1TD</th>\n",
       "      <th>RH_Count</th>\n",
       "      <th>TD_Count</th>\n",
       "      <th>EV</th>\n",
       "      <th>DTW</th>\n",
       "      <th>TWI</th>\n",
       "      <th>Suscept_3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.974714</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.049737</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.351578</td>\n",
       "      <td>2289.101516</td>\n",
       "      <td>7.397089</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>54</td>\n",
       "      <td>0.990649</td>\n",
       "      <td>0.045291</td>\n",
       "      <td>0.045291</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.953922</td>\n",
       "      <td>817.599531</td>\n",
       "      <td>5.707236</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>109</td>\n",
       "      <td>1.042453</td>\n",
       "      <td>0.049527</td>\n",
       "      <td>0.049527</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.132203</td>\n",
       "      <td>843.498715</td>\n",
       "      <td>6.575277</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>110</td>\n",
       "      <td>1.040824</td>\n",
       "      <td>0.049559</td>\n",
       "      <td>0.049559</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>3.093620</td>\n",
       "      <td>673.058063</td>\n",
       "      <td>6.745586</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>127</td>\n",
       "      <td>0.941467</td>\n",
       "      <td>0.049989</td>\n",
       "      <td>0.049989</td>\n",
       "      <td>0.450</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2.555689</td>\n",
       "      <td>1470.892179</td>\n",
       "      <td>7.038356</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31109</th>\n",
       "      <td>7287</td>\n",
       "      <td>0.138706</td>\n",
       "      <td>0.110720</td>\n",
       "      <td>0.493644</td>\n",
       "      <td>0.981</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>3.039137</td>\n",
       "      <td>789.823764</td>\n",
       "      <td>5.473199</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31110</th>\n",
       "      <td>874</td>\n",
       "      <td>0.109961</td>\n",
       "      <td>0.134051</td>\n",
       "      <td>0.436636</td>\n",
       "      <td>0.981</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>2.696230</td>\n",
       "      <td>452.568107</td>\n",
       "      <td>5.457714</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31111</th>\n",
       "      <td>12377</td>\n",
       "      <td>0.136132</td>\n",
       "      <td>0.111820</td>\n",
       "      <td>0.444701</td>\n",
       "      <td>0.981</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>3.036817</td>\n",
       "      <td>1178.350459</td>\n",
       "      <td>7.788993</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31112</th>\n",
       "      <td>5243</td>\n",
       "      <td>0.126451</td>\n",
       "      <td>0.120446</td>\n",
       "      <td>0.453853</td>\n",
       "      <td>0.981</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>2.873717</td>\n",
       "      <td>841.813685</td>\n",
       "      <td>6.007597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31113</th>\n",
       "      <td>16076</td>\n",
       "      <td>0.140150</td>\n",
       "      <td>0.108480</td>\n",
       "      <td>0.505145</td>\n",
       "      <td>0.981</td>\n",
       "      <td>34</td>\n",
       "      <td>7</td>\n",
       "      <td>2.856082</td>\n",
       "      <td>1254.088953</td>\n",
       "      <td>5.451947</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>31114 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FID_   Max_1HR  Max_1HR_2  Max_1HR_72  Max_1TD  RH_Count  TD_Count  \\\n",
       "0          1  0.974714   0.049737    0.049737    0.450         4         0   \n",
       "1         54  0.990649   0.045291    0.045291    0.450         4         0   \n",
       "2        109  1.042453   0.049527    0.049527    0.450         4         0   \n",
       "3        110  1.040824   0.049559    0.049559    0.450         4         0   \n",
       "4        127  0.941467   0.049989    0.049989    0.450         4         0   \n",
       "...      ...       ...        ...         ...      ...       ...       ...   \n",
       "31109   7287  0.138706   0.110720    0.493644    0.981        34         7   \n",
       "31110    874  0.109961   0.134051    0.436636    0.981        34         7   \n",
       "31111  12377  0.136132   0.111820    0.444701    0.981        34         7   \n",
       "31112   5243  0.126451   0.120446    0.453853    0.981        34         7   \n",
       "31113  16076  0.140150   0.108480    0.505145    0.981        34         7   \n",
       "\n",
       "             EV          DTW       TWI  Suscept_3  \n",
       "0      3.351578  2289.101516  7.397089          1  \n",
       "1      2.953922   817.599531  5.707236          1  \n",
       "2      3.132203   843.498715  6.575277          1  \n",
       "3      3.093620   673.058063  6.745586          1  \n",
       "4      2.555689  1470.892179  7.038356          1  \n",
       "...         ...          ...       ...        ...  \n",
       "31109  3.039137   789.823764  5.473199          0  \n",
       "31110  2.696230   452.568107  5.457714          0  \n",
       "31111  3.036817  1178.350459  7.788993          0  \n",
       "31112  2.873717   841.813685  6.007597          0  \n",
       "31113  2.856082  1254.088953  5.451947          0  \n",
       "\n",
       "[31114 rows x 11 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除Max_1HR_TD列，Max_1TD_HR列，Time_interval列\n",
    "train_data = train_data.drop(['Max_1HR_TD', 'Max_1TD_HR', 'Time_interval'], axis=1)\n",
    "\n",
    "train_data.to_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/train_data_231017_两类事件.csv', index=False)\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>FID_</th>\n",
       "      <th>Max_1HR</th>\n",
       "      <th>Max_1HR_2</th>\n",
       "      <th>Max_1HR_72</th>\n",
       "      <th>Max_1TD</th>\n",
       "      <th>RH_Count</th>\n",
       "      <th>TD_Count</th>\n",
       "      <th>EV</th>\n",
       "      <th>DTW</th>\n",
       "      <th>TWI</th>\n",
       "      <th>Suscept_3</th>\n",
       "      <th>dataframe_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1.197311</td>\n",
       "      <td>1.365111</td>\n",
       "      <td>1.438553</td>\n",
       "      <td>1.270</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>3.449655</td>\n",
       "      <td>2032.268709</td>\n",
       "      <td>7.511351</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1.198830</td>\n",
       "      <td>1.366789</td>\n",
       "      <td>1.439640</td>\n",
       "      <td>1.270</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>3.351578</td>\n",
       "      <td>2289.101516</td>\n",
       "      <td>7.397089</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>1.198046</td>\n",
       "      <td>1.365917</td>\n",
       "      <td>1.439070</td>\n",
       "      <td>1.270</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>3.355286</td>\n",
       "      <td>2170.271564</td>\n",
       "      <td>7.548316</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>1.200266</td>\n",
       "      <td>1.368421</td>\n",
       "      <td>1.440743</td>\n",
       "      <td>1.270</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>3.489952</td>\n",
       "      <td>2261.629379</td>\n",
       "      <td>8.816202</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>1.199598</td>\n",
       "      <td>1.367657</td>\n",
       "      <td>1.440220</td>\n",
       "      <td>1.270</td>\n",
       "      <td>22</td>\n",
       "      <td>14</td>\n",
       "      <td>3.421316</td>\n",
       "      <td>2262.551560</td>\n",
       "      <td>9.006661</td>\n",
       "      <td>1</td>\n",
       "      <td>dfffffff_2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26181</th>\n",
       "      <td>17074</td>\n",
       "      <td>0.142760</td>\n",
       "      <td>0.123092</td>\n",
       "      <td>0.285944</td>\n",
       "      <td>0.851</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>6.211980</td>\n",
       "      <td>178.929795</td>\n",
       "      <td>4.136526</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26182</th>\n",
       "      <td>2010</td>\n",
       "      <td>0.121050</td>\n",
       "      <td>0.115432</td>\n",
       "      <td>0.278399</td>\n",
       "      <td>0.851</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.700394</td>\n",
       "      <td>492.013293</td>\n",
       "      <td>4.658407</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26183</th>\n",
       "      <td>9715</td>\n",
       "      <td>0.148576</td>\n",
       "      <td>0.129895</td>\n",
       "      <td>0.256156</td>\n",
       "      <td>0.851</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>3.323290</td>\n",
       "      <td>1231.248812</td>\n",
       "      <td>6.757911</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26184</th>\n",
       "      <td>16840</td>\n",
       "      <td>0.164015</td>\n",
       "      <td>0.125745</td>\n",
       "      <td>0.276208</td>\n",
       "      <td>0.851</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.258388</td>\n",
       "      <td>2205.467687</td>\n",
       "      <td>8.519953</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26185</th>\n",
       "      <td>7909</td>\n",
       "      <td>0.076514</td>\n",
       "      <td>0.040121</td>\n",
       "      <td>0.167740</td>\n",
       "      <td>0.851</td>\n",
       "      <td>15</td>\n",
       "      <td>3</td>\n",
       "      <td>2.382718</td>\n",
       "      <td>772.392312</td>\n",
       "      <td>6.501359</td>\n",
       "      <td>0</td>\n",
       "      <td>dfffffff_26</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>26186 rows × 12 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        FID_   Max_1HR  Max_1HR_2  Max_1HR_72  Max_1TD  RH_Count  TD_Count  \\\n",
       "0          0  1.197311   1.365111    1.438553    1.270        22        14   \n",
       "1          1  1.198830   1.366789    1.439640    1.270        22        14   \n",
       "2          2  1.198046   1.365917    1.439070    1.270        22        14   \n",
       "3          3  1.200266   1.368421    1.440743    1.270        22        14   \n",
       "4          4  1.199598   1.367657    1.440220    1.270        22        14   \n",
       "...      ...       ...        ...         ...      ...       ...       ...   \n",
       "26181  17074  0.142760   0.123092    0.285944    0.851        15         3   \n",
       "26182   2010  0.121050   0.115432    0.278399    0.851        15         3   \n",
       "26183   9715  0.148576   0.129895    0.256156    0.851        15         3   \n",
       "26184  16840  0.164015   0.125745    0.276208    0.851        15         3   \n",
       "26185   7909  0.076514   0.040121    0.167740    0.851        15         3   \n",
       "\n",
       "             EV          DTW       TWI  Suscept_3 dataframe_name  \n",
       "0      3.449655  2032.268709  7.511351          1     dfffffff_2  \n",
       "1      3.351578  2289.101516  7.397089          1     dfffffff_2  \n",
       "2      3.355286  2170.271564  7.548316          1     dfffffff_2  \n",
       "3      3.489952  2261.629379  8.816202          1     dfffffff_2  \n",
       "4      3.421316  2262.551560  9.006661          1     dfffffff_2  \n",
       "...         ...          ...       ...        ...            ...  \n",
       "26181  6.211980   178.929795  4.136526          0    dfffffff_26  \n",
       "26182  2.700394   492.013293  4.658407          0    dfffffff_26  \n",
       "26183  3.323290  1231.248812  6.757911          0    dfffffff_26  \n",
       "26184  2.258388  2205.467687  8.519953          0    dfffffff_26  \n",
       "26185  2.382718   772.392312  6.501359          0    dfffffff_26  \n",
       "\n",
       "[26186 rows x 12 columns]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 删除Max_1HR_TD列，Max_1TD_HR列，Time_interval列\n",
    "test_data = test_data.drop(['Max_1HR_TD', 'Max_1TD_HR', 'Time_interval'], axis=1)\n",
    "\n",
    "test_data.to_csv('D:/a. work/papers/interpretable ML for urban flood susceptibility/数据类型对比分析/test_data_231017_两类事件.csv', index=False)\n",
    "\n",
    "test_data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
